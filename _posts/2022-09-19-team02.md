---
layout: post
comments: true
title: "Exploring the Decision Transformer in Reinforcement Learning"
author: Jacob Yoke Hong Si
date: 2022-09-19
bibliography: bibliography.bib
---


> The development of Transformers in machine learning has allowed the possibilities of high-dimensional distribution models of semantic concepts at scale. However, the applications of transformers have mostly been limited to language generalization and image generation. Therefore, this article introduces the Decision Transformer (DT), an offline reinforcement learning method. The DT uses conditional sequence modelling which allows it to leverage the simplicity and scalability of the Transformer. In the DT, an autoregressive generative model is conditioned on the return, previous states and actions. This enables the DT to obtain future actions with the desired return. Through comprehensive experiments of the DT against state-of-the-art model-free offline RL baselines on OpenAI Gym, the DT remains extremely competitive and outperforms the other models. 

<!-- Tabular data are omnipresent in various sectors of industries. Though neural networks with tabular data such as TabNet have been developed recently, the interpretability of such can be improved and enhanced. This paper proposes the TabNet Dir-cVAE Model, a more interpretable deep learning architecture for tabular data. The TabNet Dir-cVAE utilizes sequential attention while incorporating a conditional Variational Autoencoder (cVAE) to improve the generative capabilities. Within our cVAE, we model our underlying tabular data using a Dirichlet prior since it is the conjugate prior to categorical distributions such as tabular data. Furthermore, a KL Divergence regularizer has been consolidated in our model to promote sparsity between subsequent feature masks to prevent overlapping feature selection which maximizes the model's efficacy. Through comprehensive experiments on two real-world datasets, we demonstrate that TabNet Dir-cVAE Model outperforms the original TabNet by simultaneously attaining effective accuracies and interpretable feature masks. -->

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
The transformer is a sequence transduction model that utilises the attention mechanism where dependencies can be modeled regardless of their length between the input and output sequences [1]. A major advantage of the transformer is that it is able to process the whole input simultaneously, enabling parallization to improve efficacy. Within the fields of NLP and CV, a transformer is much more efficient than its counterparts where it achieves state of the art performances. Hence, the Decision Transformer (DT) aims to leverage the proficiencies of the transformer and extend its applications to the areas of reinforcement learning.

In the DT, the transformer is trained with the input of state, action and return-to-go (sum of future rewards) to create a generative trajectory model that maximises the expected return. Utilising this architecture does not require us bootstrap or discount future rewards. This is greatly beneficial since we are able to avoid the "deadly triad" [2] where the value function suffers from instabilities as well as short-sightness of the model respectively. Furthermore, through the self-attention of a transformer, credit can be assigned directly to update the weights compared to Bellman backups where the parameterization of the model is slow and can be affected by "distractor" signals (unrelated "distractor" task containing incidental rewards that affects the model's decision making). Last but not least, we model our DT as an offline reinforcement learning problem, meaning that the agent does not interact with the environment and only utilizes data collected from other agents or human demonstrations [4]. With offline RL, we are able to learn maximally effective policies despite only using limited data. Furthermore, the model reduce the use of policy sampling due to the autoregressive generative nature of the model.

Hence, with the aforementioned pros of the transformer, we expect the DT to train efficiently, generalize reliably and optimize accurately in a reinforcement learning setting.

## Related Works

## Decision Transformer 

### Transformer architecture

## Experiments and Discussions
I evaluate the performance of the Decision Transformer against other state-of-the-art model-free methods using the OpenAI Gym. The hyperparameters used suggested by the authors are as follows. 

- Batch Size = $64$
- Dropout = $0.1$
- Learning Rate = $0.0001$
- Weight Decay = $0.0001$
- Gradient Norm Clipping = $0.25$
- Return-to-go conditioning (HalfCheetah) = $6000$
- Return-to-go conditioning (Hopper) = $3600$
- Return-to-go conditioning (Walker) = $5000$

The dataset used in the experiments is the "Medium-Replay" dataset. It is the replay buffer of an agent trained to the performance of a medium policy.

The model-free methods that will be compared against include CQL [5], BEAR [6], BRAC [7], and AWR [8]. The values are reported from the Decision Transformer paper. The results of the expected returns are as follows.

| Dataset       | Environment | DT (my run) | DT (authors' run) |   CQL   | BEAR   | BRAC-v |  AWR   |
| :---          |    :----:   |    :---:    |       :---:       |  :---:  |  :---: |  :---: |  ---:  | 
| Medium-Replay | HalfCheetah |   $45.0$    |      $36.6$       | $46.2$  | $38.6$ | $\textbf{47.7}$ | $40.3$ |
| Medium-Replay | Hopper      |   $27.3$    |      $\textbf{82.7}$       | $48.6$  | $33.7$ | $0.6$  | $28.4$ | 
| Medium-Replay | Walker      |   $28.7$    |      $\textbf{66.6}$       | $26.7$  | $19.2$ | $0.9$  | $15.5$ | 
 
As shown in the table above, due to limited computational power and larger datasets of Hopper and Walker, I am unable achieve similar performance results. On the other hand, my result achieved from HalfCheetah $(45.0)$ is competitive against the SOTA BRAC-v $(47.7)$ and better than the authors' result of $36.6$.

<figure align="center">
  <img width="90%" src="../../../assets/images/team02/cheetah.png">
  <img width="90%" src="../../../assets/images/team02/hopper.png">
  <img width="90%" src="../../../assets/images/team02/walker.png">
  <figcaption>Fig 1. Average sampled return accumulated by the agent vs. specified target returns.</figcaption>
</figure>

## Basic Syntax
### Image
Please create a folder with the name of your team id under `/assets/images/`, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/team00/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:

$$
\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}
$$

or you can write in-text formula $$y = wx + b$$.

### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/).

## Reference
Please make sure to cite properly in your work, for example:

[1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).

[2] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, (2018).

[3] Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico
Carnevale, Arun Ahuja, and Greg Wayne. Optimizing agent behavior over long time scales by transporting value. Nature communications, 10(1):1–12, (2019).

[4] Beeching, Edward, and Thomas Simonini. “Introducing Decision Transformers on Hugging Face.” Hugging Face – The AI Community Building the Future., 28 Mar. 2022, https://huggingface.co/blog/decision-transformers. 

[5] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems, (2020).

[6] Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.

[7] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, (2019).

[8] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.